<div align="center">

<img src="https://capsule-render.vercel.app/api?type=waving&color=0:7c3aed,50:3b82f6,100:06b6d4&height=220&section=header&text=Zee-AI&fontSize=80&fontColor=fff&animation=fadeIn&fontAlignY=35&desc=Self-Hosted%20AI%2FLLM%20Platform&descAlignY=55&descSize=20" alt="Zee-AI Banner" width="100%"/>

<br />

<p>
  <img src="https://img.shields.io/badge/Go-1.23-00ADD8?style=for-the-badge&logo=go&logoColor=white" alt="Go"/>
  <img src="https://img.shields.io/badge/Next.js-16-black?style=for-the-badge&logo=next.js" alt="Next.js"/>
  <img src="https://img.shields.io/badge/Ollama-Latest-white?style=for-the-badge&logo=ollama" alt="Ollama"/>
  <img src="https://img.shields.io/badge/Docker-Ready-2496ED?style=for-the-badge&logo=docker&logoColor=white" alt="Docker"/>
  <img src="https://img.shields.io/badge/TypeScript-5.x-3178C6?style=for-the-badge&logo=typescript&logoColor=white" alt="TypeScript"/>
  <img src="https://img.shields.io/badge/License-MIT-green?style=for-the-badge" alt="License"/>
</p>

<h3>ğŸš€ A powerful, beautiful, self-hosted AI/LLM platform</h3>

<p>
  Run AI models locally. Your data stays on your machine. Forever.
</p>

</div>

---

## âœ¨ Features

- âš¡ **Streaming Chat** â€” Real-time token-by-token response via SSE
- ğŸ§  **Multi-Model Support** â€” Switch between any Ollama model instantly
- ğŸ’¬ **Conversation History** â€” Full chat persistence with SQLite
- ğŸ¨ **Premium Dark UI** â€” Glassmorphism, smooth animations, responsive design
- ğŸ“¦ **Model Management** â€” Pull, list, and delete models from the UI
- ğŸ”’ **100% Privacy** â€” Everything runs locally, zero data leaves your device
- ğŸ³ **Docker Ready** â€” One command to deploy the entire stack
- ğŸ“Š **Smart Titles** â€” AI auto-generates conversation titles
- âŒ¨ï¸ **Keyboard Shortcuts** â€” Enter to send, Shift+Enter for newline
- ğŸ“‹ **Copy Code Blocks** â€” One-click copy for AI responses
- ğŸŒŠ **Markdown Rendering** â€” Tables, code blocks, lists, and more

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Client                        â”‚
â”‚              Next.js 16 + React 19               â”‚
â”‚         Zustand + Framer Motion + SSE            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ HTTP + SSE
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Go API Server                   â”‚
â”‚         net/http + SQLite + SSE Streaming         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ HTTP (Ollama API)
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ollama                         â”‚
â”‚         Local LLM Runtime (GPU/CPU)              â”‚
â”‚    Gemma â€¢ Llama â€¢ Mistral â€¢ DeepSeek â€¢ etc.     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology | Purpose |
|:---|:---|:---|
| **Frontend** | Next.js 16, React 19, TypeScript | UI Framework |
| **Styling** | Tailwind CSS 4, Framer Motion | Design System & Animations |
| **State** | Zustand | Client State Management |
| **Backend** | Go 1.23, net/http | API Server |
| **Database** | SQLite (WAL mode) | Conversation Persistence |
| **AI Engine** | Ollama | Local LLM Runtime |
| **Streaming** | SSE (Server-Sent Events) | Real-time Token Delivery |
| **Deployment** | Docker, Docker Compose | Containerization |

---

## ğŸš€ Quick Start

### Prerequisites

- **Go** 1.23+ â†’ [Download](https://go.dev/dl/)
- **Node.js** 20+ & **pnpm** â†’ [Download](https://nodejs.org/)
- **Ollama** â†’ [Download](https://ollama.com/download)

### 1. Start Ollama & Pull a Model

```bash
# Start Ollama server
ollama serve

# Pull a model (in another terminal)
ollama pull gemma3
```

### 2. Clone & Setup

```bash
git clone https://github.com/ifauzeee/Zee-AI.git
cd Zee-AI

# Copy environment file
cp .env.example .env
```

### 3. Run Backend

```bash
# Install Go dependencies
go mod tidy

# Run the API server
go run ./cmd/server/
```

### 4. Run Frontend

```bash
cd web

# Install dependencies
pnpm install

# Start dev server
pnpm dev
```

### 5. Open

Visit **http://localhost:3000** and start chatting! ğŸ‰

---

## ğŸ³ Docker Deployment

```bash
# One command to rule them all
docker compose up -d --build

# Services:
# - Ollama:   http://localhost:11434
# - API:      http://localhost:8080
# - Frontend: http://localhost:3000
```

### GPU Support (NVIDIA)

Uncomment the GPU section in `docker-compose.yml`:

```yaml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: [gpu]
```

---

## ğŸ“ Project Structure

```
Zee-AI/
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ server/
â”‚       â””â”€â”€ main.go              # Entry point
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ router.go            # HTTP router & middleware
â”‚   â”‚   â””â”€â”€ handlers.go          # API handlers (chat, models, convos)
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ config.go            # Environment config
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â””â”€â”€ database.go          # SQLite layer
â”‚   â””â”€â”€ ollama/
â”‚       â””â”€â”€ client.go            # Ollama API client
â”œâ”€â”€ web/                         # Next.js Frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”œâ”€â”€ layout.tsx       # Root layout
â”‚   â”‚   â”‚   â”œâ”€â”€ page.tsx         # Main page
â”‚   â”‚   â”‚   â””â”€â”€ globals.css      # Design system
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ Sidebar.tsx      # Sidebar with convos & models
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatArea.tsx     # Main chat interface
â”‚   â”‚   â”‚   â””â”€â”€ MessageBubble.tsx # Message component
â”‚   â”‚   â””â”€â”€ lib/
â”‚   â”‚       â”œâ”€â”€ api.ts           # API client & types
â”‚   â”‚       â””â”€â”€ store.ts         # Zustand store
â”‚   â””â”€â”€ Dockerfile               # Frontend Docker
â”œâ”€â”€ .env.example                  # Environment template
â”œâ”€â”€ docker-compose.yml            # Full stack Docker
â”œâ”€â”€ Dockerfile                    # Go API Docker
â”œâ”€â”€ go.mod
â””â”€â”€ README.md
```

---

## ğŸ”Œ API Endpoints

| Method | Endpoint | Description |
|:---|:---|:---|
| `GET` | `/api/health` | Health check (Ollama + API status) |
| `GET` | `/api/models` | List available Ollama models |
| `POST` | `/api/models/pull` | Pull a new model (SSE progress) |
| `DELETE` | `/api/models/{name}` | Delete a model |
| `GET` | `/api/conversations` | List all conversations |
| `POST` | `/api/conversations` | Create new conversation |
| `GET` | `/api/conversations/{id}` | Get conversation with messages |
| `PATCH` | `/api/conversations/{id}` | Update conversation title |
| `DELETE` | `/api/conversations/{id}` | Delete conversation |
| `POST` | `/api/chat` | Chat with AI (SSE streaming) |
| `GET` | `/api/stats` | Usage statistics |

---

## ğŸ¤ Supported Models

Any model available on [Ollama](https://ollama.com/library) works with Zee-AI:

| Model | Size | Best For |
|:---|:---|:---|
| `gemma3` | 3.9 GB | General purpose, fast |
| `llama3.2` | 2 GB | Lightweight, fast responses |
| `mistral` | 4.1 GB | Balanced quality & speed |
| `deepseek-r1` | 4.7 GB | Reasoning, code |
| `codellama` | 3.8 GB | Code generation |
| `qwen2.5` | 4.7 GB | Multilingual support |

```bash
# Pull any model
ollama pull gemma3
ollama pull deepseek-r1
ollama pull codellama
```

---

## ğŸ“œ License

This project is licensed under the [MIT License](LICENSE).

---

<div align="center">

Made with âš¡ by **Muhammad Ibnu Fauzi**

[â¬† Back to Top](#zee-ai)

</div>
